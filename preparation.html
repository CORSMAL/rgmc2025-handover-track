<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="EN" lang="EN" dir="ltr">
<head profile="http://gmpg.org/xfn/11"> 
	<link rel="shortcut icon" href="images/favicon.ico" /> 
	
	<!-- Global site tag (gtag.js) - Google Analytics --> 
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-134177032-1"></script>
	<script> window.dataLayer = window.dataLayer  || []; 
	function gtag(){dataLayer.push(arguments);} 
	gtag('js', new Date()); 

gtag('config', 'UA-134177032-1'); </script>

<title>
	CORSMAL | Human-to-Robot Handovers track | 10th Robotics Grasping and Manipulation Competition | ICRA 2025
</title>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" /> 
<meta http-equiv="imagetoolbar" content="no" />
<meta http-equiv="KeyWords" content="CORSMAL, robotics, vision, audio, signal processing, competition, challenge, ICRA, human-to-robot, handovers, 2025"/>
<meta name="image" property="og:image" content="/images/CORSMAL_logo.png">

<link rel="stylesheet" href="css/layout.css" type="text/css" /> 
<link rel="stylesheet" href="css/teams.css" type="text/css" /> 

<script type="text/javascript" src="js/jquery-1.4.1.min.js"></script>  
<script type="text/javascript" src="js/jquery.slidepanel.setup.js"></script> 
<script type="text/javascript" src="js/jquery-ui-1.7.2.custom.min.js"></script>
<script type="text/javascript" src="js/jquery.tabs.setup.js"></script>

<style>
	
	#menu_div
	{
		position: -webkit-sticky; /* Safari */
		position: sticky;
		top: 0;
		background: #fff;
	}
	
	/* NAVIGATION */
	nav {0
		width: 100%;
		margin: 0 auto;
		background: #fff;
		position: static;
	}

	.fixed-header {
		position: fixed;
		top: 0;
		left: 0;
		width: 100%; 
	}

	/* By Dominik Biedebach @domobch */
	nav ul {
		list-style: none;
		text-align: center;
	}
	nav ul li {
		display: inline-block;
	}
	nav ul li a {
		display: block;
		padding: 15px;
		text-decoration: none;
		color: #aaa;
		font-weight: 300;
		margin: 0 10px;
	}
	nav ul li a,
	nav ul li a:after,
	nav ul li a:before {
		transition: all .5s;
	}
	nav ul li a:hover {
		color: #555;
	}

	/* stroke */
	nav.stroke ul li a,
	nav.fill ul li a {
		position: relative;
	}
	nav.stroke ul li a:after,
	nav.fill ul li a:after {
		position: absolute;
		bottom: 0;
		left: 0;
		right: 0;
		margin: auto;
		width: 0%;
		content: '.';
		color: transparent;
		background: #333;
		height: 1px;
	}
	nav.stroke ul li a:hover:after {
		width: 100%;
	}


	#teams {
		list-style-type: none;
		margin: 0;
		padding: 0;
		width: 200px;
		background-color: #f1f1f1;
	}

	#teams li a {
		display: block;
		color: #000;
		padding: 8px 16px;
		text-decoration: none;
	}

	/* Change the link color on hover */
	#teams li a:hover {
		background-color: #555;
		color: white;
	}

	#dates, #description, #leaderboard, #evaluation, #rules,
	#documentation {
		display: block;
		margin-top: -50px;
		padding-top: 50px;
	}

	.hide {
		display: none;
	}

	tr:hover {background-color: #f5f5f5;}
	
	.tg  {border-collapse:collapse;border-spacing:0;}
/*	.tg td{border-color:black;border-style:solid;border-width:1px;overflow:hidden;padding:10px 5px;word-break:normal;}*/
		.tg th{border-color:black;border-style:solid;border-width:1px;font-weight:normal;font-size:14.0pt;overflow:hidden;padding:10px 5px;word-break:normal;}
			.tg .tg-km2t{border-color:#ffffff;font-weight:bold;font-size:14.0pt;text-align:left;vertical-align:top}
			.tg .tg-zv4m{border-color:#ffffff;font-size:14.0pt;text-align:left;vertical-align:top}


	
	.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
	  overflow:hidden;padding:10px 5px;word-break:normal;}
	.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
	  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
	
	.tg .tg-ekg0{background-color:#EFEFEF;font-weight:bold;text-align:left;vertical-align:top}
	.tg .tg-xt05{background-color:#D9D9D9;text-align:left;vertical-align:top}
	.tg .tg-0lax{text-align:left;vertical-align:top}
	.tg .tg-ktyi{text-align:left;vertical-align:top}
	.tg .tg-xqm4{background-color:#D9D9D9;font-weight:bold;text-align:left;vertical-align:top}

</style>


<script type="text/javascript" src="js/jquery-1.4.1.min.js"></script>  
<script type="text/javascript" src="js/jquery.slidepanel.setup.js"></script> 
<script type="text/javascript" src="js/jquery-ui-1.7.2.custom.min.js"></script>
<script type="text/javascript" src="js/jquery.tabs.setup.js"></script>

<link href="../benchmark/dist/css/tabulator.min.css" rel="stylesheet">
<script type="text/javascript" src="../benchmark/dist/js/tabulator.min.js"></script>
<link href="https://unpkg.com/tabulator-tables@4.4.3/dist/css/tabulator.min.css" rel="stylesheet">
<script type="text/javascript" src="https://unpkg.com/tabulator-tables@4.4.3/dist/js/tabulator.min.js"></script>

<script type="text/javascript" src="../js/leaderboards.js"></script>


</head>
<body id="top">
	<div class="wrapper row1">
		<div id="header" class="clear">
			<div class="fl_left" style="width:33%">
				<ul>
					<li>
						<p>
							<a href="https://corsmal.eecs.qmul.ac.uk/"><img src="images/CORSMAL_logo.png" style="padding:0px 0px 0px 0px;height:75px" alt="CORSMAL"/></a>
						</p>
					</li>
				</ul>
			</div>
			<div style="float:left;width: 33%;margin-top: 12pt;text-align: center;">
				<span><a href="https://2025.ieee-icra.org/" TARGET = "_BLANK"><img src="images/ICRA-ATL-logo.webp" style="width:65%"></a></span>
			</div>
			<div class="fl_right" style="margin-top:12pt;width: 33%;text-align: right;">
				<b><span style="font-size:12.0pt"><a href="https://sites.google.com/view/rgmc2025" TARGET = "_BLANK">10th Robotic Grasping and<br>Manipulation Competition</a></span></b>
			</div>
		</div>
	</div>
	<!--2###################################################################################################### --> 
	<div class="wrapper row2"; style="margin-bottom: 7px;">
		<div class="rnd"> <!-- ###### --> 
			<!-- ###### --> 
		</div>
	</div>
	<!-- 3####################################################################################################### --> 
	<div class="wrapper row1"> 
		<div id="container" class="clear">
			<div id="latestnewspage" class="clear">
				<div style="text-align:center;text-justify:inter-ideograph;margin-bottom:0px">
					<span style="font-size:16.0pt;"><b>Human-to-Robot Handovers</b></span>
				</div>
				<div id="topnav" style="text-align:center;"> 
				<ul>
					<li><a href="index.html"><b>Overview</b></a></li> 
					<li><a href="task.html"><b>Task</b></a></li>
					<li><a href="competition.html"><b>Competition</b></a></li>
					<li><a href="qualification.html"><b>Qualification</b></a></li>
					<li class="active"><a href="preparation.html"><b>Preparation</b></a>
					<li><a href="rules.html"><b>Rules</b></a></li> 
					<li><a href="evaluation.html"><b>Evaluation</b></a></li>
				</ul>
			</div>
			<div>
				<p>
					We provide details, instructions, and documentation for preparing the solution and trials of the handovers in your own lab.
				</p>
				<br>
			</div>
			<div id="setting-up-instructions">
				<b><span style="font-size:14.0pt">Setting up instructions</span></b>
				<br>
				<p>
					The <i>setup</i> includes a robotic arm with at least 6 degrees of freedom (e.g., UR5, KUKA) and equipped with a 2-finger parallel gripper (e.g., Robotiq 2F-85); a table where the handover is happening as well as where the robot is placed; selected containers and contents; up to two cameras (e.g., Intel RealSense D435i); and a digital scale to weigh the container. The table is covered by a <a href="https://amzn.to/2X5qnaN" title="Purchase link"><u>white table-cloth</u></a>. The two cameras should be placed at 40 cm from the robotic arm, e.g. using tripods, and oriented in such a way that they both view the centre of the table. The illustration below represents the layout in 3D of the setup within a space of 4.5 x 4.5 meters.
				</p>
				<div style="text-align: center;">
					<img src="images/rgmc10-icra25-handover-3Dmodel.webp"/ style="width: 75%">							
				</div>
				<br>
				<ul>
					<li>Teams must prepare the sensing setup such that the cameras are synchronised, calibrated and localised with respect to a calibration board. We recommend the cameras recording RGB sequences at 30 Hz with a resolution of 1280 Ã— 720 pixels (based on the setup used in the CORSMAL Benchmark).</li>
					<li>Teams should verify the behaviour of the robotic arm prior to the execution of the task (e.g., end-effector, speed, kinematics, etc.)</li>
					<li>Teams will prepare all configurations with their corresponding container and filling before starting the task. </li>
					<li>Teams must weigh the mass of the container and content, if any, for each configuration before and after executing the handover to the robot, using a weight scale.</li>
					<li>A volunteer from the team will be the person who will hand the container over to the robot using a random/natural grasp for each configuration. </li>
					<li>Any initial robot pose can be chosen with respect to the environment setup; however, the subject is expected to stand on the opposite side of the table with respect to the robot.</li>
				</ul>
			<p>
				These instructions have been revised from the <a href="https://corsmal.eecs.qmul.ac.uk/benchmark/resources/RAL-SI-2020-B19-0835-V1.0.pdf" target="_BLANK"><u>CORSMAL Human-to-Robot Handover Benchmark document</u></a>.
			</p>
			<br>
			</div>
			<div id="starting-kit">
				<b><span style="font-size:14.0pt">Starting kit and documentation</span></b>
				<br>
				<br>
				<b><span style="font-size:12.0pt">Reference publications</span></b>
				<p>
					<i>Benchmark for human-to-robot handovers of unseen containers with unknown filling</i><br>
					R. Sanchez-Matilla, K. Chatzilygeroudis, K., A. Modas, N.F. Duarte, A., Xompero, A., P. Frossard, A. Billard, A. Cavallaro<br>
					IEEE Robotics and Automation Letters, 5(2), pp.1642-1649, 2020<br>
					[<a href="https://doi.org/10.1109/LRA.2020.2969200" target="_BLANK"><u>Open Access</u></a>]
				</p>
				<p>
					<i>The CORSMAL benchmark for the prediction of the properties of containers</i><br>
					A. Xompero, S. Donaher, V. Iashin, F. Palermo, G. Solak, C. Coppola, R. Ishikawa, Y. Nagao, R. Hachiuma, Q. Liu, F. Feng, C. Lan, R. H. M. Chan, G. Christmann, J. Song, G. Neeharika, C. K. T. Reddy, D. Jain, B. U. Rehman, A. Cavallaro<br>
					IEEE Access, vol. 10, 2022.
					[<a href="https://doi.org/10.1109/ACCESS.2022.3166906" target="_BLANK"><u>Open Access</u></a>]
				</p>
				<p>
					<i>Towards safe human-to-robot handovers of unknown containers</i><br>
					Y. L. Pang, A. Xompero, C. Oh, A. Cavallaro<br>
					IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), Virtual, 8-12 Aug 2021<br>
					[<a href="https://doi.org/10.1109/RO-MAN50785.2021.9515350" target="_BLANK"><u>Open Access</u></a>]
					[<a href="https://github.com/CORSMAL/safe_handover/" target="_BLANK"><u>code</u></a>]
					[<a href="http://corsmal.eecs.qmul.ac.uk/safe_handover.html" target="_BLANK"><u>webpage</u></a>]
				</p>
				<p>
					<i>The CORSMAL Challenge contains perception solutions for the estimation of the physical properties of manipulated objects prior to a handover to a robot arm.</i><br>
					[<a href="../challenge.html"><u>challenge</u></a>]
					[<a href="https://doi.org/10.1109/ACCESS.2022.3166906" target="_BLANK"><u>paper 1</u></a>]
					[<a href="https://doi.org/10.48550/arXiv.2203.01977" target="_BLANK"><u>paper 2</u></a>]
				</p>
				<p>
					<i>Additional references</i><br>
					[<a href="https://corsmal.eecs.qmul.ac.uk/resources/challenge/Additional_References.pdf" target="_BLANK"><u>document</u></a>]
				</p>	
				<br>
				<b><span style="font-size:12.0pt">Reference software</span></b>
				<p>
					<i>Vision baseline</i><br>
					A vision-based algorithm, part of a larger system, proposed for localising, tracking and estimating the dimensions of a container with a stereo camera.<br>
					[<a href="https://ieeexplore.ieee.org/document/8968407/" target="_BLANK"><u>paper</u></a>]
					[<a href="https://github.com/CORSMAL/Benchmark" target="_BLANK"><u>code</u></a>]
					[<a href="http://corsmal.eecs.qmul.ac.uk/benchmark.html" target="_BLANK"><u>webpage</u></a>]
				</p>
				<p>
					<i>LoDE</i><br>
					A method that jointly localises container-like objects and estimates their dimensions with a generative 3D sampling model and a multi-view 3D-2D iterative shape fitting, using two wide-baseline, calibrated RGB cameras.<br>
					[<a href="https://arxiv.org/abs/1911.12354" target="_BLANK"><u>paper</u></a>]
					[<a href="https://github.com/CORSMAL/LoDE" target="_BLANK"><u>code</u></a>]
					[<a href="http://corsmal.eecs.qmul.ac.uk/LoDE.html" target="_BLANK"><u>webpage</u></a>]
				</p>
				<br>
				<b><span style="font-size:12.0pt">Reference data</span></b>
				<p>
				<i>CORSMAL Containers Manipulation (1.0) [Data set]</i><br>
  				A. Xompero, R. Sanchez-Matilla, R. Mazzon, and A. Cavallaro<br>
  				Queen Mary University of London. <a href="https://doi.org/10.17636/101CORSMAL1"><u>https://doi.org/10.17636/101CORSMAL1</u></a><br>
  			</p>
			</div>
		</div>
	</div>
	<!-- footer --> </div>  <!--added to clear error with content element --> 
</body>
</html>

